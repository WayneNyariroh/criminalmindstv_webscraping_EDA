{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54176ee-30b4-4c6c-a8db-cc5213b26fb4",
   "metadata": {},
   "source": [
    "# Web Scraping TV Show Data from Various Sources for Data Analysis and Visualization using Python\n",
    "##### By Wayne Omondi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f088df4-9ac1-4b06-9910-aee593e451bc",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7cf1a8-301b-4602-a4dd-f5f33a364ca4",
   "metadata": {},
   "source": [
    "***Web scraping*** is the process of extracting and parsing data from websites. It's a useful technique for creating your own datasets for research and learning. The scraping process involves 'downloading', parsing and processing HTML documents from our target pages.<br> The steps to take will be:\n",
    "\n",
    "- Picking a websites and identifying the information to scrape from the site based on our objective(s).\n",
    "- Using the requests library to 'get' web page(s) locally\n",
    "- Inspecting the webpage's HTML source and knowing the tags that contains the information we seek.\n",
    "- Using Beautiful Soup to parse (break into components) and extracting relevant information from the html document into a dataframe.\n",
    "- Cleaning our data and data types, and some feature engineering where necessary.\n",
    "- (optional)Exporting our scraped datasets into relevant CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec8d68-68d5-437b-869e-87510e3acc9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "For this project the target TV Show is **Criminal Minds**, one of my personal favourites. In my opinion, the show did 'fall-off' in the later seasons and I'd like to see if the data speaks to that and the overall data on the show and its perfomances during the seasons it aired off (16 in total).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6a11e-91bc-4ccc-8bc3-72b74014689a",
   "metadata": {
    "tags": []
   },
   "source": [
    "![!](images/Screenshot2022-10-25212751.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1101e-5ca8-4992-aa0e-99aa7f2ffa21",
   "metadata": {
    "tags": []
   },
   "source": [
    "![!](images/Screenshot2022-10-25212823.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715681c7-38f0-4c5d-92c4-7440157ac63d",
   "metadata": {},
   "source": [
    "The data for the TV show will come from IMDB and Wikipedia. IMDB will include a Summary, Ratings and Votes for each episode, while the Wikipedia page will contain the Viewers (in millions) for each episodes: we will create a dataframe from both websites and then merge them into one dataset with all the data we need for analysis and vizualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d3457-cd8a-4694-a22a-d090a6e01e9e",
   "metadata": {},
   "source": [
    "### 1.0: Libraries/Tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab5696d-d656-47d3-836a-2918d6185153",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml --quiet\n",
    "!pip install requests --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a9d292-4c4a-4d55-900c-18793dce9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get() send a GET request to the specified url\n",
    "#bs4 lib for pulling data out of HTML/XML files\n",
    "#pandas for data processing and data manipulation\n",
    "\n",
    "from requests import get \n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c7286f-fcde-4463-bebc-d46d20bb1a2d",
   "metadata": {},
   "source": [
    "### 2.0: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9075931-7fbb-4a5e-a34b-8aab773ac2d2",
   "metadata": {},
   "source": [
    "#### 2.1: Scraping Data from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db397e1-5ab2-4b84-9396-f903142ced25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our first target website is wikipedia\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/List_of_Criminal_Minds_episodes' \n",
    "\n",
    "#list of criminalminds' episodes on wikipedia. the data we need here is in a table hence read_html() will be a great option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa83242b-15c4-4195-8d29-acebe01d0968",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using read_html() method to get the tabular data for the html doc\n",
    "wiki_html = pd.read_html(wiki_url)\n",
    "\n",
    "#view the first two rows of the first table for the html document\n",
    "wiki_html[0].head(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1953c-d03f-4ee9-8b56-df72f72fb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many tables are in the doc\n",
    "len(wiki_html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78366a6e-bb1e-4c37-bcec-f407afcd97e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#iterate through all table elements to view them so we see the ones we want based on their index\n",
    "for i, t in enumerate(wiki_html): \n",
    "    print(\"***********************************\") #a separator between each table element\n",
    "    \n",
    "    #show the index and table\n",
    "    print(i) \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c3233-2467-4e1a-9589-928882ddd3aa",
   "metadata": {},
   "source": [
    "Based on the table outputs, we want indices 1 to 15 which should cover seasons 1 to 15 of the show.<br> \n",
    "While at it we can see that the table with the last season (15) had a different column name than the previous. It is _'U.S. viewers (millions)'_ while the rest are 'US viewers (millions)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204440f6-721f-408b-b985-72905e9e93db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#based on the above output season 15 of criminalminds is index 15\n",
    "wiki_html[15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e946187-34d5-46c3-bc63-e3203b75c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the column and make change permanent in the dataframe\n",
    "wiki_html[15].rename(columns={\n",
    "    'U.S. viewers (millions)':'US viewers (millions)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c656fe-56bd-43e9-9de0-c96b4e268282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the tables we need and append them\n",
    "#empty list for our resulting data\n",
    "cm_wiki_data = []\n",
    "\n",
    "#range from season 1 to 15 (indices 1, 15)\n",
    "for i in range(1,16):\n",
    "    cm_wiki_data.append(wiki_html[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b388f4d-4060-4725-a179-7abb43913db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_wiki_df = pd.concat(cm_wiki_data)\n",
    "cm_wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eaedaa-c7f3-4b86-a63e-4f6cfbbca065",
   "metadata": {},
   "source": [
    "We now have all the relevant data from the wikipedia page compiled into a single dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be6269-9448-4941-a602-232057e49d31",
   "metadata": {},
   "source": [
    "#### 2.2: Scraping Data from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701830b-a4f1-4e3f-a12b-8c41597457d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list that will compose our dataframe\n",
    "season_number_lst = []\n",
    "episode_number_lst = []\n",
    "episode_title_lst = []\n",
    "episode_description_lst = []\n",
    "imdb_rating_lst = []\n",
    "imdb_votes_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261d25c-6f9c-4ef8-b620-d6a5731dfb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#retrieving the html documents for each season's page from imdb\n",
    "#criminal minds has 15 seasons\n",
    "for season in range(15):\n",
    "    season_number = season + 1\n",
    "    #print(f'--Extracting Data for Season {season_number}')\n",
    "    imdb_url = f'https://www.imdb.com/title/tt0452046/episodes?season={season_number}' \n",
    "    \n",
    "    #each season as its own page hence the 'season=' with our variable\n",
    "    imdb_response = get(imdb_url)\n",
    "    \n",
    "    #response.status_code - 200 is connection established \n",
    "\n",
    "    season_html = BeautifulSoup(imdb_response.content)\n",
    "    season_info = season_html.findAll('div', attrs={\n",
    "        'class':'info'})\n",
    "    \n",
    "    #retrieving on the relevant data from each season's retrieve html docs\n",
    "    for episode_number, episode in enumerate(season_info):\n",
    "        episode_title = episode.strong.a.text\n",
    "        #print(f'episode title: {episode_title}')\n",
    "        \n",
    "        episode_description = episode.find(attrs={\n",
    "            'class':'item_description'}).text\n",
    "        #print(f'summary: {episode_description}')\n",
    "        \n",
    "        imdb_rating = episode.find(attrs={\n",
    "            'class':'ipl-rating-star__rating'}).text\n",
    "        #print(f'episode name: {imdb_rating}')\n",
    "        \n",
    "        imdb_votes = episode.find(attrs={\n",
    "            'class':'ipl-rating-star__total-votes'}).text\n",
    "        #print(f'votes on imdb: {imdb_votes}')\n",
    "        \n",
    "        #print(f'\\n')\n",
    "        \n",
    "        season_number_lst.append(season_number)\n",
    "        episode_number_lst.append(episode_number + 1)\n",
    "        episode_title_lst.append(episode_title)\n",
    "        episode_description_lst.append(episode_description)\n",
    "        imdb_rating_lst.append(imdb_rating)\n",
    "        imdb_votes_lst.append(imdb_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae2620-e3c1-44d3-8400-c3cd41a05a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe using our outputs\n",
    "cm_imdb_df = pd.DataFrame({\n",
    "        'season_number':season_number_lst,\n",
    "        'episode_number':episode_number_lst,\n",
    "        'episode_title':episode_title_lst,\n",
    "        'episode_description':episode_description_lst,\n",
    "        'imdb_rating':imdb_rating_lst,\n",
    "        'imdb_votes':imdb_votes_lst\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc575746-8042-4c06-84ef-bb10e77fff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_imdb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99087ebd-2782-4232-8534-f9eb5b2da03b",
   "metadata": {},
   "source": [
    "### 3.0: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f374f4-30d0-497c-9ef3-433f39686c84",
   "metadata": {},
   "source": [
    "We will use some string methods like .strip() and .replace() for remove punctuation marks from the values where none is needed.<br>\n",
    "Drop some column(s)<br>\n",
    "Converting string data type features into numeric features - for the features that we will need for calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b7ce1-8d0c-4e37-b60b-112851eb2a4a",
   "metadata": {},
   "source": [
    "#### 3.1: Cleaning the Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878244ed-41eb-40e1-bb98-ff2bcc9b93ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#info on our first 2 table (season 1)\n",
    "cm_wiki_df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34ad13-4957-43ea-8bb9-802210f74e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_wiki_df.drop(columns = 'Prod. code', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce0665a-aa50-4a58-9806-a866a2ea79a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove quotation marks for the Titles\n",
    "cm_wiki_df.Title = cm_wiki_df['Title'].str.strip('\"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c4d56-cf07-4d62-87b4-a3a0e09220db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clean 'US viewers (millions)' column\n",
    "#get the first 4 characters\n",
    "cm_wiki_df['US viewers (millions)'] = [x[:5] for x in cm_wiki_df['US viewers (millions)']]\n",
    "cm_wiki_df['US viewers (millions)'] = cm_wiki_df['US viewers (millions)'].str.strip('[')\n",
    "\n",
    "cm_wiki_df['US viewers (millions)'] = pd.to_numeric(cm_wiki_df['US viewers (millions)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c382f-b1e7-41cf-af1e-186f7aa7fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_wiki_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee2cd8-3677-4d6f-98ba-1aff01a11792",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_wiki_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20567f-ac51-4835-a778-aa9692109425",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_wiki_df.rename(columns={\n",
    "    \"Title\": \"episode_title\",\n",
    "    \"No. overall\":\"episode_number_overall\",\n",
    "    \"No. in season\":\"episode_number\",\n",
    "    \"Directed by\":\"episode_director\",\n",
    "    \"Written by\":\"episode_writer\",\n",
    "    \"Original air date\":\"episode_airdate\",\n",
    "    \"US viewers (millions)\":\"us_viewers_in_millions\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78344f9f-bf44-4f19-8f70-9ca53053b23a",
   "metadata": {},
   "source": [
    "#### 3.2: Cleaning the IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a7cd5-8a91-4444-a692-fef006939a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#checking out data types\n",
    "cm_imdb_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0e013-236b-4ca8-958f-184f3b208bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up the string features\n",
    "cm_imdb_df.episode_description = cm_imdb_df['episode_description'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2fb45-e5cf-4ace-9859-4ce6f9042412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data types \n",
    "cm_imdb_df['imdb_votes'] = cm_imdb_df['imdb_votes'].str.strip('()').str.replace(\",\", \"\").astype(int)\n",
    "\n",
    "cm_imdb_df.imdb_rating = pd.to_numeric(cm_imdb_df.imdb_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6b269-4074-4318-b37c-a60d34eb61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632bbd82-aa6d-4386-9127-3e91ddae76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_imdb_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f15ea3-42c7-4500-b99a-ec68b03f8007",
   "metadata": {},
   "source": [
    "our imdb dataframe have 323 episodes, while the wikipedia dataframe has 324 episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d5122-922f-4001-a5ef-e52cd74cb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check the number of episodes per season in the imdb df\n",
    "cm_imdb_df.groupby(['season_number'])['episode_number'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3aed06-bba5-4905-8e3d-081d6e474c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first table with seasons overview on wikipedia\n",
    "wiki_html[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7b2e7-ed13-44ce-b744-ff1b59e28fa6",
   "metadata": {},
   "source": [
    "season 4 has 25 episodes in one df and 26 in another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968c601-2920-43b4-be6b-eddcec7edc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_html[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf42e9-8ab1-4c0c-bb3c-2fc0ab763772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view season 4 in the imdb df\n",
    "cm_imdb_df[cm_imdb_df['season_number']==4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103c76ca-d40f-4ab7-9c95-dc1f9b396b97",
   "metadata": {},
   "source": [
    "As a fan of the show I remember that in the original airing, Episodes 25 & 26 of season was aired as one 2 hour long episode \"To Hell...And Back\". On wikipedia it is split as two separate episodes 25. To Hell & 26. And Back; while IMDB regards it as one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd02a4-c612-45c2-b2dd-3f683e0f83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to delete the row that contains the split part in the wikipedia df\n",
    "cm_wiki_df = cm_wiki_df[~cm_wiki_df.episode_name.str.contains(\"And Back\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2d370-7309-42f8-9e9b-be13cf2dfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then change episode name of episode 5 in the imdb df\n",
    "cm_wiki_df['episode_name'] = cm_wiki_df['episode_name'].replace(\"To Hell\",\"To Hell... And Back\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc64bf-985a-408a-bee9-df9a7e07ddca",
   "metadata": {},
   "source": [
    "### 4.0: Combining Our Dataframes Into One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f2a03-159c-436f-86fe-4442ea98c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_imdb_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b3b96-b622-454b-a95d-83a4727f5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_wiki_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191f05f-b4b3-4b63-b276-78b0cb96dd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
