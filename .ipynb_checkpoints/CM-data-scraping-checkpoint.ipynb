{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e54176ee-30b4-4c6c-a8db-cc5213b26fb4",
   "metadata": {},
   "source": [
    "# Web Scraping TV-Show Data from Various Sources for Data Analysis and Visualization using Python\n",
    "##### By Wayne Omondi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f088df4-9ac1-4b06-9910-aee593e451bc",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7cf1a8-301b-4602-a4dd-f5f33a364ca4",
   "metadata": {},
   "source": [
    "***Web scraping*** is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating your own datasets for research and learning. The scraping process involves 'downloading', parsing and processing HTML documents from our target pages.<br> The steps to take will be:\n",
    "\n",
    "- Picking a websites and identifying the information to scrape from the site based on our objective(s).\n",
    "- Using the requests library to 'get' web page(s) locally\n",
    "- Inspecting the webpage's HTML source and knowing the tags that contains the information we seek.\n",
    "- Using Beautiful Soup to parse (break into components) and extracting relevant information from the html document into a dataframe.\n",
    "- Cleaning our data and data types, and some feature engineering where necessary.\n",
    "- (optional)Exporting our scraped datasets into relevant CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec8d68-68d5-437b-869e-87510e3acc9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "For this project the target TV Show is **Criminal Minds**, one of my personal favourites. In my opinion, the show did 'fall-off' in the later seasons and I'd like to see if the data speaks to that and the overall data on the show and its perfomances during the seasons it aired off (16 in total).<br>\n",
    "The data for the TV show will come from IMDB and Wikipedia. IMDB will include a Summary, Ratings and Votes for each episode, while the Wikipedia page will contain the Viewers (in millions) for each episodes: we will create a dataframe from both websites and then merge them into one dataset with all the data we need for analysis and vizualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e6a11e-91bc-4ccc-8bc3-72b74014689a",
   "metadata": {
    "tags": []
   },
   "source": [
    "![!](images/Screenshot2022-10-25212751.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1101e-5ca8-4992-aa0e-99aa7f2ffa21",
   "metadata": {
    "tags": []
   },
   "source": [
    "![!](images/Screenshot2022-10-25212823.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d3457-cd8a-4694-a22a-d090a6e01e9e",
   "metadata": {},
   "source": [
    "### 1.0: The 'Tools' We Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5696d-d656-47d3-836a-2918d6185153",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml --quiet\n",
    "!pip install requests --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9d292-4c4a-4d55-900c-18793dce9264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get() send a GET request to the specified url\n",
    "#bs4 lib for pulling data out of HTML/XML files\n",
    "#pandas for data processing and data manipulation\n",
    "\n",
    "from requests import get \n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9075931-7fbb-4a5e-a34b-8aab773ac2d2",
   "metadata": {},
   "source": [
    "### 2.0: Scraping Data from the First Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db397e1-5ab2-4b84-9396-f903142ced25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our first target website is wikipedia\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/List_of_Criminal_Minds_episodes' \n",
    "\n",
    "#list of criminalminds' episodes on wikipedia. the data we need here is in a table hence read_html() will be a great option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83242b-15c4-4195-8d29-acebe01d0968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#using read_html() method to get the tabular data for the html doc\n",
    "wiki_html = pd.read_html(wiki_url)\n",
    "\n",
    "#view the first two rows of the first table for the html document\n",
    "wiki_html[0].head(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1953c-d03f-4ee9-8b56-df72f72fb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many tables are in the doc\n",
    "len(wiki_html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78366a6e-bb1e-4c37-bcec-f407afcd97e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#iterate through all table elements to view them so we see the ones we want based on their index\n",
    "for i, t in enumerate(wiki_html): \n",
    "    print(\"***********************************\") #a separator between each table element\n",
    "    \n",
    "    #show the index and table\n",
    "    print(i) \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204440f6-721f-408b-b985-72905e9e93db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#based on the above output season 1 of criminalminds is index 1\n",
    "wiki_html[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c656fe-56bd-43e9-9de0-c96b4e268282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the tables we need and append them\n",
    "# empty list for our resulting data\n",
    "cm_wiki_data = []\n",
    "\n",
    "# range from season 1 to 15 (indices 1, 15)\n",
    "for i in range(1,16):\n",
    "    cm_wiki_data.append(wiki_html[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b388f4d-4060-4725-a179-7abb43913db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_wiki_df = pd.concat(cm_wiki_data)\n",
    "cm_wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eaedaa-c7f3-4b86-a63e-4f6cfbbca065",
   "metadata": {},
   "source": [
    "We now have all the relevant data from the wikipedia page compiled into a single dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61b7ce1-8d0c-4e37-b60b-112851eb2a4a",
   "metadata": {},
   "source": [
    "#### 2.1: Cleaning the Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878244ed-41eb-40e1-bb98-ff2bcc9b93ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#info on our first 2 table (season 1)\n",
    "cm_wiki_df.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551fa5a6-c004-439f-89a9-fbe82ce10a8d",
   "metadata": {},
   "source": [
    "from the dataframe we can already some columns that will need  to be dropped, cleaned, and some that will need to be assigned a new datatypes, for example _'US viewers (millions)'_ and _'Original air date'_<br>\n",
    "The last two columns also have to be merged into one column, the table with the last season (15) had a different column name than the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34ad13-4957-43ea-8bb9-802210f74e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08be6269-9448-4941-a602-232057e49d31",
   "metadata": {},
   "source": [
    "### 3.0: Scraping Data from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701830b-a4f1-4e3f-a12b-8c41597457d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list that will compose our dataframe\n",
    "season_number_lst = []\n",
    "episode_number_lst = []\n",
    "episode_name_lst = []\n",
    "episode_description_lst = []\n",
    "episode_airdate_lst = []\n",
    "imdb_rating_lst = []\n",
    "imdb_votes_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261d25c-6f9c-4ef8-b620-d6a5731dfb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieving the html documents for each season's page from imdb\n",
    "# criminal minds has 15 seasons\n",
    "for season in range(15):\n",
    "    season_number = season + 1\n",
    "#    print(f'Extracting Data for Season {season_number}')\n",
    "    imdb_url = f'https://www.imdb.com/title/tt0452046/episodes?season={season_number}' #each season as its own page hence the 'season=' with our variable\n",
    "    imdb_response = get(imdb_url)\n",
    "    \n",
    "#   response.status_code - 200 is connection established \n",
    "\n",
    "    season_html = BeautifulSoup(imdb_response.content)\n",
    "    season_info = season_html.findAll('div', attrs={'class':'info'})\n",
    "    \n",
    "# retrieving on the relevant data from each season's retrieve html docs\n",
    "    for episode_number, episode in enumerate(season_info):\n",
    "        episode_name = episode.strong.a.text\n",
    "#        print(f'episode name: {episode_name}')\n",
    "        \n",
    "        episode_description = episode.find(attrs={'class':'item_description'})\n",
    "#        print(f'summary: {episode_description}')\n",
    "        \n",
    "        episode_airdate = episode.find(attrs={'class':'airdate'}).text.strip()\n",
    "#        print(f'episode aired on: {episode_airdate}')\n",
    "        \n",
    "        imdb_rating = episode.find(attrs={'class':'ipl-rating-star__rating'}).text\n",
    "#        print(f'episode name: {imdb_rating}')\n",
    "        \n",
    "        imdb_votes = episode.find(attrs={'class':'ipl-rating-star__total-votes'}).text.strip('()')\n",
    "#        print(f'votes on imdb: {imdb_votes}')\n",
    "        \n",
    "#        print(f'\\n')\n",
    "        \n",
    "        season_number_lst.append(season_number)\n",
    "        episode_number_lst.append(episode_number + 1)\n",
    "        episode_name_lst.append(episode_name)\n",
    "        episode_description_lst.append(episode_description)\n",
    "        episode_airdate_lst.append(episode_airdate)\n",
    "        imdb_rating_lst.append(imdb_rating)\n",
    "        imdb_votes_lst.append(imdb_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae2620-e3c1-44d3-8400-c3cd41a05a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_imdb_df = pd.DataFrame({\n",
    "        'season_number':season_number_lst,\n",
    "        'episode_number':episode_number_lst,\n",
    "        'episode_name':episode_name_lst,\n",
    "        'episode_description':episode_description_lst,\n",
    "        'imdb_rating':imdb_rating_lst,\n",
    "        'imdb_votes':imdb_votes_lst\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc575746-8042-4c06-84ef-bb10e77fff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b509c-3617-44cf-8593-1001e23b0237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99087ebd-2782-4232-8534-f9eb5b2da03b",
   "metadata": {},
   "source": [
    "#### 3.1: Cleaning the IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a7cd5-8a91-4444-a692-fef006939a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking out data types\n",
    "cm_imdb_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2fb45-e5cf-4ace-9859-4ce6f9042412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \",\" in 'imdb_votes' \n",
    "cm_imdb_df.imdb_votes = cm_imdb_df['imdb_votes'].str.replace(\",\",\"\")\n",
    "\n",
    "# converting the column data type to allow math operations\n",
    "cm_imdb_df.imdb_votes = pd.to_numeric(cm_imdb_df.imdb_votes)\n",
    "\n",
    "# converting the ratings column as well\n",
    "cm_imdb_df.imdb_rating = pd.to_numeric(cm_imdb_df.imdb_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632bbd82-aa6d-4386-9127-3e91ddae76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_imdb_df.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f23d2-47a0-47ff-90ee-113237116957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
